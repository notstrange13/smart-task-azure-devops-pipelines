# Smart Task Example: Intelligent Test Strategy Decision
# This pipeline demonstrates how AI can analyze code changes and decide which tests to run

trigger:
  branches:
    include:
      - main
      - develop
      - feature/*

variables:
  - name: MODEL_TYPE
    value: 'AZURE_OPENAI'
  - name: AZURE_OPENAI_INSTANCE_NAME
    value: 'your-openai-instance'
  - name: AZURE_OPENAI_KEY
    value: '$(AZURE_OPENAI_API_KEY)' # Store as secret variable
  - name: AZURE_OPENAI_DEPLOYMENT_NAME
    value: 'gpt-4o'
  - name: AZURE_OPENAI_API_VERSION
    value: '2024-02-15-preview'

pool:
  vmImage: 'ubuntu-latest'

stages:
  - stage: IntelligentTestStrategy
    displayName: 'AI-Driven Test Strategy Decision'
    jobs:
      - job: AnalyzeAndDecide
        displayName: 'Analyze Code Changes and Decide Test Strategy'
        steps:
          - checkout: self
            displayName: 'Checkout Source Code'

          - task: SmartTask@1
            displayName: 'AI Decision: Intelligent Test Strategy'
            inputs:
              prompt: |
                Analyze this repository and the current build context to make intelligent testing decisions:

                1. **Analyze the codebase structure:**
                   - Read package.json to understand the project type and dependencies
                   - Check if this is a frontend, backend, or full-stack project
                   - Identify testing frameworks in use

                2. **Analyze the build context:**
                   - Check the source branch name to understand the type of changes
                   - Determine if this is a feature, hotfix, or main branch build
                   - Get build reason (manual, PR, scheduled, etc.)

                3. **Make testing decisions based on analysis:**
                   - For main/master branch: Run full test suite
                   - For feature branches: Run relevant tests based on likely changed areas
                   - For hotfix branches: Run critical path tests only
                   - For dependency updates: Run integration tests

                4. **Set pipeline variables for the test strategy:**
                   - RUN_UNIT_TESTS: true/false
                   - RUN_INTEGRATION_TESTS: true/false
                   - RUN_E2E_TESTS: true/false
                   - TEST_SCOPE: 'minimal' | 'standard' | 'comprehensive'
                   - TEST_REASON: explanation of the decision

                Use the available tools to gather information and make data-driven decisions.

              mode: 'decision'
              additionalContext: |
                {
                  "project_context": {
                    "type": "web_application",
                    "environments": ["development", "staging", "production"],
                    "test_types": ["unit", "integration", "e2e", "performance"]
                  },
                  "decision_criteria": {
                    "main_branch": "comprehensive testing required",
                    "feature_branch": "targeted testing based on changes",
                    "hotfix_branch": "critical path testing only",
                    "scheduled_build": "full regression testing"
                  }
                }

          - task: PowerShell@2
            displayName: 'Display AI Test Strategy Decisions'
            inputs:
              script: |
                Write-Host "=== AI Test Strategy Decisions ==="
                Write-Host "Source Branch: $(Build.SourceBranchName)"
                Write-Host "Build Reason: $(Build.Reason)"
                Write-Host ""
                Write-Host "=== AI Recommendations ==="
                Write-Host "Run Unit Tests: $(RUN_UNIT_TESTS)"
                Write-Host "Run Integration Tests: $(RUN_INTEGRATION_TESTS)"
                Write-Host "Run E2E Tests: $(RUN_E2E_TESTS)"
                Write-Host "Test Scope: $(TEST_SCOPE)"
                Write-Host "Decision Reason: $(TEST_REASON)"

  - stage: ExecuteTests
    displayName: 'Execute AI-Recommended Tests'
    dependsOn: IntelligentTestStrategy
    condition: succeeded()
    jobs:
      - job: UnitTests
        displayName: 'Unit Tests'
        condition: eq(variables['RUN_UNIT_TESTS'], 'true')
        steps:
          - checkout: self
          - task: NodeTool@0
            inputs:
              versionSpec: '18.x'
          - script: npm ci
            displayName: 'Install dependencies'
          - script: npm run test:unit
            displayName: 'Run Unit Tests'

      - job: IntegrationTests
        displayName: 'Integration Tests'
        condition: eq(variables['RUN_INTEGRATION_TESTS'], 'true')
        steps:
          - checkout: self
          - task: NodeTool@0
            inputs:
              versionSpec: '18.x'
          - script: npm ci
            displayName: 'Install dependencies'
          - script: npm run test:integration
            displayName: 'Run Integration Tests'

      - job: E2ETests
        displayName: 'End-to-End Tests'
        condition: eq(variables['RUN_E2E_TESTS'], 'true')
        steps:
          - checkout: self
          - task: NodeTool@0
            inputs:
              versionSpec: '18.x'
          - script: npm ci
            displayName: 'Install dependencies'
          - script: npm run test:e2e
            displayName: 'Run E2E Tests'
